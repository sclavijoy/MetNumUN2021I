{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "sclavijoytutorial5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfLc8zkWv_Q"
      },
      "source": [
        "#***Pandas-Resampling and DataFrame***\n",
        "\n",
        "#Introduction\n",
        "In the last chapter we had a glimpse of Pandas. In this chapter we will learn about resampling methods and the DataFrame object, which is a powerful tool for financial data analysis.\n",
        "\n",
        "#Fetching Data\n",
        "Here we use the Quandl API to retrieve data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZdNMc53UbQM",
        "outputId": "4356d306-bdc4-46fc-8ec7-1575e905688d"
      },
      "source": [
        "! pip install quandl\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quandl in /usr/local/lib/python3.7/dist-packages (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.1.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from quandl) (8.7.0)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from quandl) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from quandl) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->quandl) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14->quandl) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-t_CQ18W5AH"
      },
      "source": [
        "import quandl\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvi5wsTJW9vC"
      },
      "source": [
        "We will create a Series named \"aapl\" whose values are Apple's daily closing prices, which are of course indexed by dates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LitwbGeNVT35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "6f50bcc0-e559-41bf-92aa-f2ec539ecac6"
      },
      "source": [
        "quandl.ApiConfig.api_key = 'dRQxJ15_2nrLznxr1Nn4'\n",
        "aapl_table = quandl.get('WIKI/AAPL')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f10443357830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dRQxJ15_2nrLznxr1Nn4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maapl_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WIKI/AAPL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/get.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_column_not_found\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/model/dataset.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mupdated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mupdated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle_not_found_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/operations/list.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(cls, **options)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                        **options)\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mhandle_api_error\u001b[0;34m(cls, resp)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuandlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m: (Status 400) (Quandl Error QEAx01) We could not recognize your API key. Please check your API key and try again."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXdgfk4vVT36"
      },
      "source": [
        "aapl = aapl_table['Adj. Close']['2017']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSzkwzkjVT36"
      },
      "source": [
        "print(aapl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffZL87egXG-K"
      },
      "source": [
        "Recall that we can fetch a specific data point using series['yyyy-mm-dd']. We can also fetch the data in a specific month using series['yyyy-mm']."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWMaPK8CVT37"
      },
      "source": [
        "print(aapl['2017-3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpCrea0-XH62"
      },
      "source": [
        "Or in several consecutive months:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ16IZ2aVT38"
      },
      "source": [
        "aapl['2017-2':'2017-4']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJwU4AvLXL7F"
      },
      "source": [
        ".head(N) and .tail(N) are methods for quickly accessing the first or last N elements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4XD3anQVT38"
      },
      "source": [
        "print(aapl.head(5))\n",
        "print(aapl.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBgxp5NcXUb0"
      },
      "source": [
        "#Resampling\n",
        "series.resample(freq) is a class called \"DatetimeIndexResampler\" which groups data in a Series object into regular time intervals. The argument \"freq\" determines the length of each interval.\n",
        "\n",
        "series.resample.mean() is a complete statement that groups data into intervals, and then compute the mean of each interval. For example, if we want to aggregate the daily data into monthly data by mean:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_qjddprVT38"
      },
      "source": [
        "by_month = aapl.resample('M').mean()\n",
        "print(by_month)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVPTBmjBXZIv"
      },
      "source": [
        "We can also aggregate the data by week:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jL7twp8VT39"
      },
      "source": [
        "by_week = aapl.resample('W').mean()\n",
        "print(by_week.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFDcAubXO3Cv",
        "outputId": "cbcdaf8b-2737-4d79-c31f-1e7296037550"
      },
      "source": [
        "aapl.resample('M').max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2017-01-31    120.443739\n",
              "2017-02-28    135.999390\n",
              "2017-03-31    142.952608\n",
              "2017-04-30    143.597342\n",
              "2017-05-31    155.469192\n",
              "2017-06-30    154.821818\n",
              "2017-07-31    152.839860\n",
              "2017-08-31    164.000000\n",
              "2017-09-30    164.050000\n",
              "2017-10-31    169.040000\n",
              "2017-11-30    175.880000\n",
              "2017-12-31    176.420000\n",
              "Freq: M, Name: Adj. Close, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Res6IlvRXc4e"
      },
      "source": [
        "We can choose almost any frequency by using the format 'nf', where 'n' is an integer and 'f' is M for month, W for week and D for day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRJj0MT_VT3-"
      },
      "source": [
        "three_day = aapl.resample('3D').mean()\n",
        "two_week = aapl.resample('2W').mean()\n",
        "two_month = aapl.resample('2M').mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V86Nx2s8Xy1R"
      },
      "source": [
        "Besides the mean() method, other methods can also be used with the resampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2OxQNvkVT3-"
      },
      "source": [
        "std = aapl.resample('W').std()\n",
        "max = aapl.resample('W').max()\n",
        "min = aapl.resample('W').min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8llgk_GYaUW"
      },
      "source": [
        "Often we want to calculate monthly returns of a stock, based on prices on the last day of each month. To fetch those prices, we use the series.resample.agg() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6bYRMwqVT3_"
      },
      "source": [
        "last_day = aapl.resample('M').agg(lambda x: x[-1])\n",
        "print(last_day)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXKIj1UacVpE"
      },
      "source": [
        "Or directly calculate the monthly rates of return using the data for the first day and the last day:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1vWSbcXVT4A"
      },
      "source": [
        "monthly_return = aapl.resample('M').agg(lambda x: x[-1]/x[1] - 1)\n",
        "monthly_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzEUn3jKctVi"
      },
      "source": [
        "Series object also provides us some convenient methods to do some quick calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chwkqIgwVT4A"
      },
      "source": [
        "print(monthly_return.mean())\n",
        "print(monthly_return.std())\n",
        "print(monthly_return.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Y32P5Zc0TS"
      },
      "source": [
        "Another two methods frequently used on Series are .diff() and .pct_change(). The former calculates the difference between consecutive elements, and the latter calculates the percentage change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZSEJ1odVT4B"
      },
      "source": [
        "print(last_day.diff())\n",
        "print(last_day.pct_change())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYilsJX0dEIU"
      },
      "source": [
        "Notice that we induced a NaN value while calculating percentage changes i.e. returns.\n",
        "\n",
        "When dealing with NaN values, we usually either removing the data point or fill it with a specific value. Here we fill it with 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5EKHSxkVT4B"
      },
      "source": [
        "daily_return = last_day.pct_change()\n",
        "print(daily_return.fillna(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liZRWWCgdKr8"
      },
      "source": [
        "Alternatively, we can fill a NaN with the next fitted value. This is called 'backward fill', or 'bfill' in short:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsnOdMghVT4C"
      },
      "source": [
        "daily_return = last_day.pct_change()\n",
        "print(daily_return.fillna(method = 'bfill'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5BBlFqydSg8"
      },
      "source": [
        "As expected, since there is a 'backward fill' method, there must be a 'forward fill' method, or 'ffill' in short. However we can't use it here because the NaN is the first value.\n",
        "\n",
        "We can also simply remove NaN values by .dropna()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Le7QHNkVT4C"
      },
      "source": [
        "daily_return = last_day.pct_change()\n",
        "daily_return.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlQX24CdW2z"
      },
      "source": [
        "#DataFrame\n",
        "The DataFrame is the most commonly used data structure in Pandas. It is essentially a table, just like an Excel spreadsheet.\n",
        "\n",
        "More precisely, a DataFrame is a collection of Series objects, each of which may contain different data types. A DataFrame can be created from various data types: dictionary, 2-D numpy.ndarray, a Series or another DataFrame.\n",
        "\n",
        "#Create DataFrames\n",
        "The most common method of creating a DataFrame is passing a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQzCPjYFVT4D"
      },
      "source": [
        "dict = {'AAPL': [143.5, 144.09, 142.73, 144.18, 143.77],'GOOG':[898.7, 911.71, 906.69, 918.59, 926.99],\n",
        "        'IBM':[155.58, 153.67, 152.36, 152.94, 153.49]}\n",
        "data_index = pd.date_range('2017-07-03',periods = 5, freq = 'D')\n",
        "df = pd.DataFrame(dict, index = data_index)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc33Yu1Z9I2"
      },
      "source": [
        "#Manipulating DataFrames\n",
        "We can fetch values in a DataFrame by columns and index. Each column in a DataFrame is essentially a Pandas Series. We can fetch a column by square brackets: df['column_name']\n",
        "\n",
        "If a column name contains no spaces, then we can also use df.column_name to fetch a column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VBFOTsdVT4E"
      },
      "source": [
        "df = aapl_table\n",
        "print(df.Close.tail(5))\n",
        "print(df['Adj. Volume'].tail(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoIRMNHGaEeV"
      },
      "source": [
        "All the methods we applied to a Series index such as iloc[], loc[] and resampling methods, can also be applied to a DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wn36R_kVT4E"
      },
      "source": [
        "aapl_2016 = df['2016']\n",
        "aapl_month = aapl_2016.resample('M').agg(lambda x: x[-1])\n",
        "print(aapl_month)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzGptUQ9aI-3"
      },
      "source": [
        "We may select certain columns of a DataFrame using their names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqEJ1N7RVT4E"
      },
      "source": [
        "aapl_bar = aapl_month[['Open', 'High', 'Low', 'Close']]\n",
        "print(aapl_bar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaBTt1QUaL3W"
      },
      "source": [
        "We can even specify both rows and columns using loc[]. The row indices and column names are separated by a comma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueREO26sVT4E"
      },
      "source": [
        "print(aapl_month.loc['2016-03':'2016-06',['Open', 'High', 'Low', 'Close']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsNLmxdDaRFt"
      },
      "source": [
        "The subset methods in DataFrame is quite useful. By writing logical statements in square brackets, we can make customized subsets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KcXAC5NVT4F"
      },
      "source": [
        "above = aapl_bar[aapl_bar.Close > np.mean(aapl_bar.Close)]\n",
        "print(above)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EtLo0vJaU-E"
      },
      "source": [
        "#Data Validation\n",
        "As mentioned, all methods that apply to a Series can also be applied to a DataFrame. Here we add a new column to an existing DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su0RWDRIVT4F"
      },
      "source": [
        "aapl_bar['rate_return'] = aapl_bar.Close.pct_change()\n",
        "print(aapl_bar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQyWqQUnafT2"
      },
      "source": [
        "Here the calculation introduced a NaN value. If the DataFrame is large, we would not be able to observe it. isnull() provides a convenient way to check abnormal values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8U-5vaoVT4F"
      },
      "source": [
        "missing = aapl_bar.isnull()\n",
        "print(missing)\n",
        "print('\\n------------------ separate line -----------------\\n')\n",
        "print(missing.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSMe2I1mal8N"
      },
      "source": [
        "The row labelled \"unique\" indicates the number of unique values in each column. Since the \"rate_return\" column has 2 unique values, it has at least one missing value.\n",
        "\n",
        "We can deduce the number of missing values by comparing \"count\" with \"freq\". There are 12 counts and 11 False values, so there is one True value which corresponds to the missing value.\n",
        "\n",
        "We can also find the rows with missing values easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Yd7Sj0VT4G"
      },
      "source": [
        "print(missing[missing.rate_return == True])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qm9lAOvapJb"
      },
      "source": [
        "Usually when dealing with missing data, we either delete the whole row or fill it with some value. As we introduced in the Series chapter, the same method dropna() and fillna() can be applied to a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQS5Hks6VT4G"
      },
      "source": [
        "drop = aapl_bar.dropna()\n",
        "print(drop)\n",
        "print('\\n---------------------- separate line--------------------\\n')\n",
        "fill = aapl_bar.fillna(0)\n",
        "print(fill)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyp4UsXeatdd"
      },
      "source": [
        "#DataFrame Concat\n",
        "We have seen how to extract a Series from a dataFrame. Now we need to consider how to merge a Series or a DataFrame into another one.\n",
        "\n",
        "In Pandas, the function concat() allows us to merge multiple Series into a DataFrame:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h049bFgeVT4G"
      },
      "source": [
        "s1 = pd.Series([143.5, 144.09, 142.73, 144.18, 143.77], name = 'AAPL')\n",
        "s2 = pd.Series([898.7, 911.71, 906.69, 918.59, 926.99], name = 'GOOG')\n",
        "data_frame = pd.concat([s1,s2], axis = 1)\n",
        "print(data_frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJGHKI9GazMl"
      },
      "source": [
        "The \"axis = 1\" parameter will join two DataFrames by columns:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juk2__LoVT4H"
      },
      "source": [
        "log_price = np.log(aapl_bar.Close)\n",
        "log_price.name = 'log_price'\n",
        "print(log_price)\n",
        "print('\\n---------------------- separate line--------------------\\n')\n",
        "concat = pd.concat([aapl_bar, log_price], axis = 1)\n",
        "print(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGvRa3Dua26V"
      },
      "source": [
        "We can also join two DataFrames by rows. Consider these two DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgR-3lNMVT4H"
      },
      "source": [
        "df_volume = aapl_table.loc['2016-10':'2017-04',['Volume', 'Split Ratio']].resample('M').agg(lambda x: x[-1])\n",
        "print(df_volume)\n",
        "print('\\n---------------------- separate line--------------------\\n')\n",
        "df_2017 = aapl_table.loc['2016-10':'2017-04',['Open', 'High', 'Low', 'Close']].resample('M').agg(lambda x: x[-1])\n",
        "print(df_2017)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXaqdA2Ma5zs"
      },
      "source": [
        "Now we merge the DataFrames with our DataFrame 'aapl_bar'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Z4hrocVT4H"
      },
      "source": [
        "concat = pd.concat([aapl_bar,df_volume],axis = 1)\n",
        "print(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKf26EIEa83f"
      },
      "source": [
        "By default the DataFrame are joined with all of the data. This default options results in zero information loss. We can also merge them by intersection, this is called 'inner join':\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WgswO4-VT4H"
      },
      "source": [
        "concat = pd.concat([aapl_bar,df_volume],axis = 1, join = 'inner')\n",
        "print(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWsVCHQ6a_H3"
      },
      "source": [
        "Only the intersection part was left if use 'inner join' method. Now let's try to append a DataFrame to another one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVIbUt15VT4I"
      },
      "source": [
        "append = aapl_bar.append(df_2017)\n",
        "print(append)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-T_lg3CbDGu"
      },
      "source": [
        "'Append' is essentially to concat two DataFrames by axis = 0, thus here is an alternative way to append:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ4HGA5yVT4I"
      },
      "source": [
        "concat = pd.concat([aapl_bar, df_2017], axis = 0)\n",
        "print(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jRl2Le5bKof"
      },
      "source": [
        "Please note that if the two DataFrame have some columns with the same column names, these columns are considered to be the same and will be merged. It's very important to have the right column names. If we change a column names here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqvRa-fRVT4I"
      },
      "source": [
        "df_2017.columns = ['Change', 'High','Low','Close']\n",
        "concat = pd.concat([aapl_bar, df_2017], axis = 0)\n",
        "print(concat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVuE_IZGbdeV"
      },
      "source": [
        "Since the column name of 'Open' has been changed, the new DataFrame has an new column named 'Change'.\n",
        "\n",
        "#Summary\n",
        "Hereby we introduced the most import part of python: resampling and DataFrame manipulation. We only introduced the most commonly used method in Financial data analysis. There are also many methods used in data mining, which are also beneficial. You can always check the Pandas official documentations for help."
      ]
    }
  ]
}